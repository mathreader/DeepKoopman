{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkarch import weight_variable, bias_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the parameters for the experiment\n",
    "params = dict()\n",
    "\n",
    "# (1) Data Parameters\n",
    "params['data_name'] = 'Pendulum'\n",
    "params['len_time'] = 51\n",
    "n = 2  # dimension of system (and input layer)\n",
    "num_initial_conditions = 5000  # per training file\n",
    "params['delta_t'] = 0.02\n",
    "\n",
    "\n",
    "# (2) Settings related to saving results\n",
    "params['folder_name'] = 'exp2_best'\n",
    "\n",
    "\n",
    "# (3) Network and Training \n",
    "params['seed'] = 17\n",
    "params['widths'] = [2, 80,  80, 80, 80, 2]\n",
    "params['dist_weights'] = ['dl', 'dl', 'dl']\n",
    "params['dist_biases'] = ['', '', '']\n",
    "params['scale'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the seed for the experiment\n",
    "#tf.set_random_seed(params['seed'])\n",
    "tf.random.set_seed(params['seed'])\n",
    "np.random.seed(params['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = np.loadtxt(('./data/%s_train1_x.csv' % (params['data_name'])), delimiter=',', dtype=np.float64)\n",
    "data_val = np.loadtxt(('./data/%s_val_x.csv' % (params['data_name'])), delimiter=',', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would like to structure the data in the following way:\n",
    "# Loop: data_orig = [x1, x2, ..., x51] -> data_x = [x1, x2, ..., x50] / data_y = [x2, x3, ..., x51]\n",
    "\n",
    "def data_generate(data_orig):\n",
    "    '''Transforms data into our learning task of predicting next step from current step.'''\n",
    "    \n",
    "    # Initialize size\n",
    "    data_size = len(data_orig)\n",
    "    num_iters = int(np.floor(data_size / params['len_time']))\n",
    "    \n",
    "    # Initialize new datasets\n",
    "    data_x = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    data_y = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    \n",
    "    # Loop to generate new datasets\n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        output_index_start = i * (params['len_time'] - 1)\n",
    "        \n",
    "        for j in range(params['len_time'] - 1):\n",
    "            data_x[output_index_start + j, :] = data_orig[input_index_start + j, :]\n",
    "            data_y[output_index_start + j, :] = data_orig[input_index_start + j + 1, :]\n",
    "    \n",
    "    return data_x, data_y\n",
    "\n",
    "def data_batch_generate(data_x, data_y, batch_size):\n",
    "    num_batches = int(np.floor(data_x.shape[0]/batch_size))\n",
    "    \n",
    "    data_batch_x = np.zeros((batch_size, num_batches, 2))\n",
    "    data_batch_y = np.zeros((batch_size, num_batches, 2))\n",
    "    \n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        data_batch_x[:,i,:] = data_x[i*batch_size:(i+1)*batch_size,:]\n",
    "        data_batch_y[:,i,:] = data_y[i*batch_size:(i+1)*batch_size,:]\n",
    "        \n",
    "    return data_batch_x, data_batch_y\n",
    "\n",
    "\n",
    "def data_extract_first_entry(data_orig):\n",
    "    '''Extracts only first entry in the data in each sequence for comparison to DeepKoopman'''\n",
    "    \n",
    "    # Initialize size\n",
    "    data_size = len(data_orig)\n",
    "    num_iters = int(np.floor(data_size / params['len_time']))\n",
    "    \n",
    "    # Initialize dataset with first entry\n",
    "    data_x = np.zeros((num_iters, 2))\n",
    "    data_y = np.zeros((num_iters, 2))\n",
    "    \n",
    "    # Only put first entry in dataset\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        \n",
    "        data_x[i, :] = data_orig[i * params['len_time'], :]\n",
    "        data_y[i, :] = data_orig[i * params['len_time'] + 1, :]\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 128;\n",
    "\n",
    "# Training Data\n",
    "data_x, data_y = data_generate(data_orig)\n",
    "data_x = tf.convert_to_tensor(data_x, dtype=tf.float64)\n",
    "data_y = tf.convert_to_tensor(data_y, dtype=tf.float64)\n",
    "\n",
    "#data_batch_x, data_batch_y = data_batch_generate(data_x, data_y, BatchSize)\n",
    "#data_batch_x = tf.convert_to_tensor(data_batch_x, dtype=tf.float64)\n",
    "#data_batch_y = tf.convert_to_tensor(data_batch_y, dtype=tf.float64)\n",
    "\n",
    "# Validation Data\n",
    "data_val_x, data_val_y = data_generate(data_val)\n",
    "data_val_x = tf.convert_to_tensor(data_val_x, dtype=tf.float64)\n",
    "data_val_y = tf.convert_to_tensor(data_val_y, dtype=tf.float64)\n",
    "\n",
    "#data_val_batch_x, data_val_batch_y = data_batch_generate(data_val_x, data_val_y, BatchSize)\n",
    "#data_val_batch_x = tf.convert_to_tensor(data_val_batch_x, dtype=tf.float64)\n",
    "#data_val_batch_y = tf.convert_to_tensor(data_val_batch_y, dtype=tf.float64)\n",
    "\n",
    "# Comparison Data\n",
    "data_val_comp_x, data_val_comp_y = data_extract_first_entry(data_val)\n",
    "data_val_comp_x = tf.convert_to_tensor(data_val_comp_x, dtype=tf.float64)\n",
    "data_val_comp_y = tf.convert_to_tensor(data_val_comp_y, dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_actual,y_pred):\n",
    "    #custom_loss=tf.keras.backend.mean(tf.math.reduce_sum(tf.square(y_actual-y_pred), axis=-1))\n",
    "    custom_loss=tf.math.reduce_sum(tf.math.reduce_sum(tf.square(y_actual-y_pred), axis=-1))\n",
    "    return custom_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inputs = keras.Input(shape=data_x.shape)\n",
    "x = layers.Dense(80, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(80, activation=\"relu\", name=\"dense_2\")(x)\n",
    "x = layers.Dense(80, activation=\"relu\", name=\"dense_3\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "'''\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(80, activation='relu', input_shape = (2,)),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(80, activation='relu', input_shape = (2,)),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=custom_loss)\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175000, 2)\n",
      "(175000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_x.shape)\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-ac6c13f6801f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/my_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2414\u001b[0;31m       \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[0;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[1;32m   1197\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[1;32m   1198\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[0;32m-> 1199\u001b[0;31m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[1;32m   1200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "\n",
    "StepsPerEpoch = round(int(data_x.shape[0])/BatchSize);\n",
    "max_time = 20*60; # run for 20 minutes\n",
    "start_time = time.time();\n",
    "\n",
    "\n",
    "while ((time.time() - start_time) < max_time): \n",
    "    # Train\n",
    "    model.trainable = True\n",
    "    model.fit(data_x, data_y, batch_size = BatchSize, epochs = 1, shuffle=True)\n",
    "    model.save_weights('./checkpoints/my_checkpoint')\n",
    "    \n",
    "    # Compute the error on the validation set\n",
    "    model.trainable = False\n",
    "    val_loss = model.evaluate(data_val_x, data_val_y, batch_size=BatchSize, steps=5)\n",
    "    print(\"Validation loss: \" + str(val_loss))\n",
    "    \n",
    "    # Compute the \n",
    "    model2.load_weights('./checkpoints/my_checkpoint')\n",
    "    val_loss_2 = model2.evaluate(data_val_comp_x, data_val_comp_y, batch_size=1, steps=5)\n",
    "    print(\"Comparison loss: \" + str(val_loss_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_network_init(widths, dist_weights, dist_biases, scale=0.1):\n",
    "    \"\"\"Initialize a feedforward neural network: a dictionary of weights and a dictionary of biases.\n",
    "\n",
    "    Arguments:\n",
    "        widths -- array or list of widths for layers of network\n",
    "        dist_weights -- array or list of strings for distributions of weight matrices\n",
    "        dist_biases -- array or list of strings for distributions of bias vectors\n",
    "        scale -- (for tn distribution of weight matrices): standard deviation of normal distribution before truncation\n",
    "        name -- string for prefix on weight matrices (default 'D' for decoder)\n",
    "\n",
    "    Returns:\n",
    "        weights -- dictionary of weights\n",
    "        biases -- dictionary of biases\n",
    "\n",
    "    Side effects:\n",
    "        None\n",
    "    \"\"\"\n",
    "    weights = dict()\n",
    "    biases = dict()\n",
    "    for i in np.arange(len(widths) - 1):\n",
    "        ind = i + 1\n",
    "        weights['W%d' % (ind)] = weight_variable([widths[i], widths[i + 1]], var_name='W%d' % (ind),\n",
    "                                                         distribution=dist_weights[ind - 1], scale=scale)\n",
    "        biases['b%d' % (ind)] = bias_variable([widths[i + 1], ], var_name='b%d' % (ind),\n",
    "                                                      distribution=dist_biases[ind - 1])\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def feedforward_network_apply(input_data, weights, biases, act_type, num_decoder_weights):\n",
    "    \"\"\"Apply a feedforward neural network to input data\n",
    "\n",
    "    Arguments:\n",
    "        input_data -- input to network\n",
    "        weights -- dictionary of weights\n",
    "        biases -- dictionary of biases\n",
    "        act_type -- string for activation type for nonlinear layers (i.e. sigmoid, relu, or elu)\n",
    "        num_weights -- number of weight matrices (layers) in the network\n",
    "\n",
    "    Returns:\n",
    "        output of decoder network applied to input prev_layer\n",
    "\n",
    "    Side effects:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_layer = input_data\n",
    "    for i in np.arange(num_weights - 1):\n",
    "        prev_layer = tf.matmul(prev_layer, weights['WD%d' % (i + 1)]) + biases['bD%d' % (i + 1)]\n",
    "        if act_type == 'sigmoid':\n",
    "            prev_layer = tf.sigmoid(prev_layer)\n",
    "        elif act_type == 'relu':\n",
    "            prev_layer = tf.nn.relu(prev_layer)\n",
    "        elif act_type == 'elu':\n",
    "            prev_layer = tf.nn.elu(prev_layer)\n",
    "\n",
    "    # apply last layer without any nonlinearity\n",
    "    return tf.matmul(prev_layer, weights['WD%d' % num_decoder_weights]) + biases['bD%d' % num_decoder_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss function\n",
    "def loss_function(data_x, data_y):\n",
    "    '''Defines loss function for the problem.'''\n",
    "    \n",
    "    data_x\n",
    "    \n",
    "    \n",
    "    # Initialize new datasets\n",
    "    data_x = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    data_y = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    \n",
    "    # Loop to generate new datasets\n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        output_index_start = i * (params['len_time'] - 1)\n",
    "        \n",
    "        for j in range(params['len_time'] - 1):\n",
    "            data_x[output_index_start + j, :] = data_val[input_index_start + j, :]\n",
    "            data_y[output_index_start + j, :] = data_val[input_index_start + j + 1, :]\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "weights, biases = feedforward_network(params['widths'], dist_weights=params['dist_weights'], \n",
    "                                    dist_biases=params['dist_biases'], scale=params['scale'])\n",
    "\n",
    "# Train the network\n",
    "output = feedforward_network_apply(data_x, weights, biases, 'relu', len(weights))\n",
    "\n",
    "# Evaluating the loss\n",
    "loss = np.linalg.norm(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
