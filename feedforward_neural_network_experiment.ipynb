{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "seventh-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "meaningful-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkarch import weight_variable, bias_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "rational-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the parameters for the experiment\n",
    "params = dict()\n",
    "\n",
    "# (1) Data Parameters\n",
    "params['data_name'] = 'Pendulum'\n",
    "params['len_time'] = 51\n",
    "n = 2  # dimension of system (and input layer)\n",
    "num_initial_conditions = 5000  # per training file\n",
    "params['delta_t'] = 0.02\n",
    "\n",
    "\n",
    "# (2) Settings related to saving results\n",
    "params['folder_name'] = 'exp2_best'\n",
    "\n",
    "\n",
    "# (3) Network and Training \n",
    "params['seed'] = 17\n",
    "params['widths'] = [2, 80, 80, 2]\n",
    "params['dist_weights'] = ['dl', 'dl', 'dl']\n",
    "params['dist_biases'] = ['', '', '']\n",
    "params['scale'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bigger-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the seed for the experiment\n",
    "tf.set_random_seed(params['seed'])\n",
    "#tf.random.set_seed(params['seed'])\n",
    "np.random.seed(params['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-cursor",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "interim-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = np.loadtxt(('./data/%s_train1_x.csv' % (params['data_name'])), delimiter=',', dtype=np.float64)\n",
    "data_val = np.loadtxt(('./data/%s_val_x.csv' % (params['data_name'])), delimiter=',', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "professional-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would like to structure the data in the following way:\n",
    "# Loop: data_orig = [x1, x2, ..., x51] -> data_x = [x1, x2, ..., x50] / data_y = [x2, x3, ..., x51]\n",
    "\n",
    "def data_generate(data_orig):\n",
    "    '''Transforms data into our learning task of predicting next step from current step.'''\n",
    "    \n",
    "    # Initialize size\n",
    "    data_size = len(data_orig)\n",
    "    num_iters = int(np.floor(data_size / params['len_time']))\n",
    "    \n",
    "    # Initialize new datasets\n",
    "    data_x = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    data_y = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    \n",
    "    # Loop to generate new datasets\n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        output_index_start = i * (params['len_time'] - 1)\n",
    "        \n",
    "        for j in range(params['len_time'] - 1):\n",
    "            data_x[output_index_start + j, :] = data_orig[input_index_start + j, :]\n",
    "            data_y[output_index_start + j, :] = data_orig[input_index_start + j + 1, :]\n",
    "    \n",
    "    return data_x, data_y\n",
    "\n",
    "\n",
    "def data_extract_first_entry(data_orig):\n",
    "    '''Extracts only first entry in the data in each sequence for comparison to DeepKoopman'''\n",
    "    \n",
    "    # Initialize size\n",
    "    data_size = len(data_orig)\n",
    "    num_iters = int(np.floor(data_size / params['len_time']))\n",
    "    \n",
    "    # Initialize dataset with first entry\n",
    "    data_x = np.zeros((num_iters, 2))\n",
    "    data_y = np.zeros((num_iters, 2))\n",
    "    \n",
    "    # Only put first entry in dataset\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        \n",
    "        data_x[i, :] = data_orig[i * params['len_time'], :]\n",
    "        data_y[i, :] = data_orig[i * params['len_time'] + 1, :]\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "executed-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "data_x, data_y = data_generate(data_orig)\n",
    "data_x = tf.convert_to_tensor(data_x, dtype=tf.float64)\n",
    "data_y = tf.convert_to_tensor(data_y, dtype=tf.float64)\n",
    "\n",
    "# Validation Data\n",
    "data_val_x, data_val_y = data_generate(data_val)\n",
    "data_val_x = tf.convert_to_tensor(data_val_x, dtype=tf.float64)\n",
    "data_val_y = tf.convert_to_tensor(data_val_y, dtype=tf.float64)\n",
    "\n",
    "# Comparison Data\n",
    "data_val_comp_x, data_val_comp_y = data_extract_first_entry(data_val)\n",
    "data_val_comp_x = tf.convert_to_tensor(data_val_comp_x, dtype=tf.float64)\n",
    "data_val_comp_y = tf.convert_to_tensor(data_val_comp_y, dtype=tf.float64)\n",
    "print(data_val_comp_x.shape)\n",
    "print(data_val_comp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "healthy-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_actual,y_pred):\n",
    "    #custom_loss=tf.keras.backend.mean(tf.math.reduce_sum(tf.square(y_actual-y_pred), axis=-1))\n",
    "    custom_loss=tf.math.reduce_sum(tf.math.reduce_sum(tf.square(y_actual-y_pred), axis=-1))\n",
    "    return custom_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "everyday-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(80, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "confidential-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=custom_loss)\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "endless-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "5/5 [==============================] - 8s 982ms/step - batch: 2.0000 - size: 1.0000 - loss: 423641.5125\n",
      "361481.71875\n",
      "2094.219970703125\n",
      "Train on 5 samples\n",
      "5/5 [==============================] - 5s 959ms/step - batch: 2.0000 - size: 1.0000 - loss: 317383.0750\n",
      "248923.375\n",
      "1442.90673828125\n",
      "Train on 5 samples\n",
      "5/5 [==============================] - 4s 922ms/step - batch: 2.0000 - size: 1.0000 - loss: 200624.7688\n",
      "128242.3828125\n",
      "746.6096801757812\n",
      "Train on 5 samples\n",
      "5/5 [==============================] - 5s 996ms/step - batch: 2.0000 - size: 1.0000 - loss: 85721.8828\n",
      "33551.78125\n",
      "195.50433349609375\n",
      "Train on 5 samples\n",
      "5/5 [==============================] - 4s 903ms/step - batch: 2.0000 - size: 1.0000 - loss: 19623.8814\n",
      "14031.76171875\n",
      "83.31536865234375\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # Train\n",
    "    model.trainable = True\n",
    "    model.fit(data_x, data_y, epochs=1, steps_per_epoch=5, shuffle=True)\n",
    "    model.save_weights('./checkpoints/my_checkpoint')\n",
    "    \n",
    "    # Compute the error on the validation set\n",
    "    model.trainable = False\n",
    "    val_loss =model.evaluate(data_val_x, data_val_y, batch_size=1, steps=5)\n",
    "    print(val_loss)\n",
    "    \n",
    "    # Compute the \n",
    "    model2.load_weights('./checkpoints/my_checkpoint')\n",
    "    val_loss_2 =model2.evaluate(data_val_comp_x, data_val_comp_y, batch_size=1, steps=5)\n",
    "    print(val_loss_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-investigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fluid-hospital",
   "metadata": {},
   "source": [
    "Constructing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sexual-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_network_init(widths, dist_weights, dist_biases, scale=0.1):\n",
    "    \"\"\"Initialize a feedforward neural network: a dictionary of weights and a dictionary of biases.\n",
    "\n",
    "    Arguments:\n",
    "        widths -- array or list of widths for layers of network\n",
    "        dist_weights -- array or list of strings for distributions of weight matrices\n",
    "        dist_biases -- array or list of strings for distributions of bias vectors\n",
    "        scale -- (for tn distribution of weight matrices): standard deviation of normal distribution before truncation\n",
    "        name -- string for prefix on weight matrices (default 'D' for decoder)\n",
    "\n",
    "    Returns:\n",
    "        weights -- dictionary of weights\n",
    "        biases -- dictionary of biases\n",
    "\n",
    "    Side effects:\n",
    "        None\n",
    "    \"\"\"\n",
    "    weights = dict()\n",
    "    biases = dict()\n",
    "    for i in np.arange(len(widths) - 1):\n",
    "        ind = i + 1\n",
    "        weights['W%d' % (ind)] = weight_variable([widths[i], widths[i + 1]], var_name='W%d' % (ind),\n",
    "                                                         distribution=dist_weights[ind - 1], scale=scale)\n",
    "        biases['b%d' % (ind)] = bias_variable([widths[i + 1], ], var_name='b%d' % (ind),\n",
    "                                                      distribution=dist_biases[ind - 1])\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def feedforward_network_apply(input_data, weights, biases, act_type, num_decoder_weights):\n",
    "    \"\"\"Apply a feedforward neural network to input data\n",
    "\n",
    "    Arguments:\n",
    "        input_data -- input to network\n",
    "        weights -- dictionary of weights\n",
    "        biases -- dictionary of biases\n",
    "        act_type -- string for activation type for nonlinear layers (i.e. sigmoid, relu, or elu)\n",
    "        num_weights -- number of weight matrices (layers) in the network\n",
    "\n",
    "    Returns:\n",
    "        output of decoder network applied to input prev_layer\n",
    "\n",
    "    Side effects:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_layer = input_data\n",
    "    for i in np.arange(num_weights - 1):\n",
    "        prev_layer = tf.matmul(prev_layer, weights['WD%d' % (i + 1)]) + biases['bD%d' % (i + 1)]\n",
    "        if act_type == 'sigmoid':\n",
    "            prev_layer = tf.sigmoid(prev_layer)\n",
    "        elif act_type == 'relu':\n",
    "            prev_layer = tf.nn.relu(prev_layer)\n",
    "        elif act_type == 'elu':\n",
    "            prev_layer = tf.nn.elu(prev_layer)\n",
    "\n",
    "    # apply last layer without any nonlinearity\n",
    "    return tf.matmul(prev_layer, weights['WD%d' % num_decoder_weights]) + biases['bD%d' % num_decoder_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss function\n",
    "def loss_function(data_x, data_y):\n",
    "    '''Defines loss function for the problem.'''\n",
    "    \n",
    "    data_x\n",
    "    \n",
    "    \n",
    "    # Initialize new datasets\n",
    "    data_x = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    data_y = np.zeros(((params['len_time']-1)*num_iters, 2))\n",
    "    \n",
    "    # Loop to generate new datasets\n",
    "    for i in range(num_iters):\n",
    "        input_index_start = i * params['len_time'] \n",
    "        output_index_start = i * (params['len_time'] - 1)\n",
    "        \n",
    "        for j in range(params['len_time'] - 1):\n",
    "            data_x[output_index_start + j, :] = data_val[input_index_start + j, :]\n",
    "            data_y[output_index_start + j, :] = data_val[input_index_start + j + 1, :]\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "living-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "weights, biases = feedforward_network(params['widths'], dist_weights=params['dist_weights'], \n",
    "                                    dist_biases=params['dist_biases'], scale=params['scale'])\n",
    "\n",
    "# Train the network\n",
    "output = feedforward_network_apply(data_x, weights, biases, 'relu', len(weights))\n",
    "\n",
    "# Evaluating the loss\n",
    "loss = np.linalg.norm(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-courtesy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
